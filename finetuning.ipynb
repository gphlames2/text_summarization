{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\braim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import  load_metric\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Model\n",
    "import nltk\n",
    "import datasets\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pyarrow as pa\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\braim/.cache\\huggingface\\hub\\models--distilgpt2\\snapshots\\ee0b29d316a6a55fd15c50c4a97aca80ab4dd66a\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\braim/.cache\\huggingface\\hub\\models--distilgpt2\\snapshots\\ee0b29d316a6a55fd15c50c4a97aca80ab4dd66a\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\braim/.cache\\huggingface\\hub\\models--distilgpt2\\snapshots\\ee0b29d316a6a55fd15c50c4a97aca80ab4dd66a\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\braim/.cache\\huggingface\\hub\\models--distilgpt2\\snapshots\\ee0b29d316a6a55fd15c50c4a97aca80ab4dd66a\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\braim/.cache\\huggingface\\hub\\models--distilgpt2\\snapshots\\ee0b29d316a6a55fd15c50c4a97aca80ab4dd66a\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Assigning <BOS> to the bos_token key of the tokenizer\n",
      "Adding <BOS> to the vocabulary\n",
      "Assigning <EOS> to the eos_token key of the tokenizer\n",
      "Adding <EOS> to the vocabulary\n",
      "Assigning <PAD> to the pad_token key of the tokenizer\n",
      "Adding <PAD> to the vocabulary\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "# tokenizer.bos_token = tokenizer.cls_token\n",
    "# tokenizer.eos_token = tokenizer.sep_token\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').cuda()\n",
    "special_tokens_dict = {'bos_token': '<BOS>', 'eos_token': '<EOS>', 'pad_token': '<PAD>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
    "                               batch_size=16, device=device,\n",
    "                               column_text=\"article\",\n",
    "                               column_summary=\"highlights\"):\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n",
    "                        padding=\"longest\", return_tensors=\"pt\")\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                         length_penalty=0.8,max_new_tokens=30, num_beams=8)\n",
    "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "\n",
    "        # Finally, we decode the generated texts,\n",
    "        # replace the  token, and add the decoded texts with the references to the metric.\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True)\n",
    "               for s in summaries]\n",
    "\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "\n",
    "\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    #  Finally compute and return the ROUGE scores.\n",
    "    score = metric.compute()\n",
    "    return score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/braim/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "Found cached dataset cnn_dailymail (C:/Users/braim/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "Found cached dataset cnn_dailymail (C:/Users/braim/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "val_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:10%]\")\n",
    "test_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "train_data = pd.DataFrame(train_data)\n",
    "val_data = pd.DataFrame(val_data)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "training_articles = train_data['article']\n",
    "test_mask = test_data['article'].str.len() <=1024\n",
    "train_mask = train_data['article'].str.len() <=1024\n",
    "val_mask = val_data['article'].str.len() <=1024\n",
    "train_data = train_data.loc[train_mask]\n",
    "val_data = val_data.loc[val_mask]\n",
    "test_data = test_data.loc[test_mask]\n",
    "validation_length = len(train_data.index)/10\n",
    "if len(val_data.index) > validation_length:\n",
    "    val_data = val_data.head(validation_length)\n",
    "else:\n",
    "    pass\n",
    "if len(test_data.index) > validation_length:\n",
    "    test_data = test_data.head(validation_length)\n",
    "else:\n",
    "    pass\n",
    "train_data = datasets.Dataset(pa.Table.from_pandas(train_data))\n",
    "val_data = datasets.Dataset(pa.Table.from_pandas(val_data))\n",
    "test_data = datasets.Dataset(pa.Table.from_pandas(test_data))\n",
    "print(len(train_data['article'][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dialogue_token_len = [len(tokenizer.encode(s) for s in train_data['article'])]\n",
    "# summary_token_len = [len(tokenizer.encode(s) for s in train_data['highlights'])]\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey='C0' )\n",
    "# axes[0].hist(dialogue_token_len, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
    "# axes[0].set_title(\"Dialogue Token Length\")\n",
    "# axes[0].set_xlabel(\"Length\")\n",
    "# axes[0].set_ylabel(\"Count\")\n",
    "# axes[1].hist(summary_token_len, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
    "# axes[1].set_title(\"Summary Token Length\")\n",
    "# axes[1].set_xlabel(\"Length\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6ce579e8b5a473bb78e7f158a410c59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\braim\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:3578: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffbac66a2bb24b2d812a1aa977436f45"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch['article'], max_length = 1024, padding='max_length',truncation = True )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['highlights'], max_length = 1024, padding='max_length',truncation = True )\n",
    "    return {\n",
    "        'input_ids' : input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n",
    "train_data_pt = train_data.map(convert_examples_to_features, batched = True)\n",
    "val_data_pt = val_data.map(convert_examples_to_features, batched = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='distilgpt', num_train_epochs=4, warmup_steps=0,\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.1, logging_steps=10,\n",
    "    evaluation_strategy='steps', eval_steps=100, save_steps=100,\n",
    "    gradient_accumulation_steps=10,\n",
    "    overwrite_output_dir=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=trainer_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=train_data_pt,\n",
    "                  eval_dataset=val_data_pt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_metric = load_metric('rouge')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\braim\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3670\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 10\n",
      "  Total optimization steps = 1468\n",
      "  Number of trainable parameters = 81914880\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1468 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-100\n",
      "Configuration saved in distilgpt\\checkpoint-100\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-100\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-100\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-100\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-100\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-200\n",
      "Configuration saved in distilgpt\\checkpoint-200\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-200\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-200\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-200\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-200\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-300\n",
      "Configuration saved in distilgpt\\checkpoint-300\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-300\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-300\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-300\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-300\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-400\n",
      "Configuration saved in distilgpt\\checkpoint-400\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-400\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-400\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-400\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-400\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-500\n",
      "Configuration saved in distilgpt\\checkpoint-500\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-500\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-500\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-600\n",
      "Configuration saved in distilgpt\\checkpoint-600\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-600\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-600\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-600\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-600\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-700\n",
      "Configuration saved in distilgpt\\checkpoint-700\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-700\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-700\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-700\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-700\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-800\n",
      "Configuration saved in distilgpt\\checkpoint-800\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-800\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-800\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-800\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-800\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-900\n",
      "Configuration saved in distilgpt\\checkpoint-900\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-900\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-900\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-900\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-900\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-1000\n",
      "Configuration saved in distilgpt\\checkpoint-1000\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-1000\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-1000\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-1100\n",
      "Configuration saved in distilgpt\\checkpoint-1100\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-1100\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-1100\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-1100\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-1100\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-1200\n",
      "Configuration saved in distilgpt\\checkpoint-1200\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-1200\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-1200\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-1200\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-1200\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-1300\n",
      "Configuration saved in distilgpt\\checkpoint-1300\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-1300\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-1300\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-1300\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-1300\\added_tokens.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: article, __index_level_0__, highlights, id. If article, __index_level_0__, highlights, id are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to distilgpt\\checkpoint-1400\n",
      "Configuration saved in distilgpt\\checkpoint-1400\\config.json\n",
      "Model weights saved in distilgpt\\checkpoint-1400\\pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt\\checkpoint-1400\\tokenizer_config.json\n",
      "Special tokens file saved in distilgpt\\checkpoint-1400\\special_tokens_map.json\n",
      "added tokens file saved in distilgpt\\checkpoint-1400\\added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=1468, training_loss=0.35639616351686315, metrics={'train_runtime': 2063.8563, 'train_samples_per_second': 7.113, 'train_steps_per_second': 0.711, 'total_flos': 3835836298690560.0, 'train_loss': 0.35639616351686315, 'epoch': 4.0})"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./distilgpt/finetuned_distilgpt2\n",
      "Configuration saved in ./distilgpt/finetuned_distilgpt2\\config.json\n",
      "Model weights saved in ./distilgpt/finetuned_distilgpt2\\pytorch_model.bin\n",
      "tokenizer config file saved in ./distilgpt/finetuned_distilgpt2\\tokenizer_config.json\n",
      "Special tokens file saved in ./distilgpt/finetuned_distilgpt2\\special_tokens_map.json\n",
      "added tokens file saved in ./distilgpt/finetuned_distilgpt2\\added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./distilgpt/finetuned_distilgpt2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./distilgpt/finetuned_distilgpt2\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n",
      "loading weights file ./distilgpt/finetuned_distilgpt2\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./distilgpt/finetuned_distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "  0%|          | 0/158 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 1/158 [00:00<00:59,  2.65it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|▏         | 2/158 [00:00<00:49,  3.17it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  2%|▏         | 3/158 [00:00<00:46,  3.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  3%|▎         | 4/158 [00:01<00:44,  3.47it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  3%|▎         | 5/158 [00:01<00:47,  3.23it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 6/158 [00:01<00:46,  3.30it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 7/158 [00:02<00:44,  3.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  5%|▌         | 8/158 [00:02<00:43,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  6%|▌         | 9/158 [00:02<00:43,  3.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  6%|▋         | 10/158 [00:02<00:42,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  7%|▋         | 11/158 [00:03<00:42,  3.49it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 12/158 [00:03<00:42,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 13/158 [00:03<00:41,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  9%|▉         | 14/158 [00:04<00:40,  3.53it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  9%|▉         | 15/158 [00:04<00:41,  3.47it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 10%|█         | 16/158 [00:04<00:40,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 11%|█         | 17/158 [00:05<00:41,  3.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 11%|█▏        | 18/158 [00:05<00:40,  3.48it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▏        | 19/158 [00:05<00:39,  3.51it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 13%|█▎        | 20/158 [00:05<00:39,  3.53it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 13%|█▎        | 21/158 [00:06<00:38,  3.52it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 14%|█▍        | 22/158 [00:06<00:38,  3.49it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 15%|█▍        | 23/158 [00:06<00:39,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 15%|█▌        | 24/158 [00:07<00:39,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 16%|█▌        | 25/158 [00:07<00:39,  3.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 16%|█▋        | 26/158 [00:07<00:38,  3.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 27/158 [00:07<00:38,  3.44it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 18%|█▊        | 28/158 [00:08<00:37,  3.47it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 18%|█▊        | 29/158 [00:08<00:37,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 19%|█▉        | 30/158 [00:08<00:36,  3.51it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|█▉        | 31/158 [00:09<00:36,  3.50it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|██        | 32/158 [00:09<00:35,  3.50it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 33/158 [00:09<00:34,  3.61it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 22%|██▏       | 34/158 [00:09<00:35,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 22%|██▏       | 35/158 [00:10<00:33,  3.62it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 23%|██▎       | 36/158 [00:10<00:34,  3.57it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 23%|██▎       | 37/158 [00:10<00:34,  3.55it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 24%|██▍       | 38/158 [00:10<00:33,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▍       | 39/158 [00:11<00:33,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 40/158 [00:11<00:33,  3.50it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 26%|██▌       | 41/158 [00:11<00:33,  3.51it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 27%|██▋       | 42/158 [00:12<00:32,  3.55it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 27%|██▋       | 43/158 [00:12<00:33,  3.48it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 28%|██▊       | 44/158 [00:12<00:32,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 28%|██▊       | 45/158 [00:12<00:31,  3.60it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 46/158 [00:13<00:31,  3.56it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 30%|██▉       | 47/158 [00:13<00:31,  3.57it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 30%|███       | 48/158 [00:13<00:31,  3.55it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 31%|███       | 49/158 [00:14<00:31,  3.49it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 32%|███▏      | 50/158 [00:14<00:31,  3.47it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 32%|███▏      | 51/158 [00:14<00:31,  3.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 52/158 [00:14<00:31,  3.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 34%|███▎      | 53/158 [00:15<00:30,  3.44it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 34%|███▍      | 54/158 [00:15<00:30,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 35%|███▍      | 55/158 [00:15<00:29,  3.49it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 35%|███▌      | 56/158 [00:16<00:29,  3.44it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 36%|███▌      | 57/158 [00:16<00:29,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 37%|███▋      | 58/158 [00:16<00:28,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 37%|███▋      | 59/158 [00:17<00:29,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 60/158 [00:17<00:28,  3.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 39%|███▊      | 61/158 [00:17<00:28,  3.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 39%|███▉      | 62/158 [00:17<00:27,  3.44it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 40%|███▉      | 63/158 [00:18<00:28,  3.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 41%|████      | 64/158 [00:18<00:27,  3.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 41%|████      | 65/158 [00:18<00:27,  3.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 66/158 [00:19<00:27,  3.30it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 67/158 [00:19<00:28,  3.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 43%|████▎     | 68/158 [00:19<00:27,  3.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 44%|████▎     | 69/158 [00:20<00:26,  3.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 44%|████▍     | 70/158 [00:20<00:26,  3.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 45%|████▍     | 71/158 [00:20<00:25,  3.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 72/158 [00:20<00:25,  3.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 73/158 [00:21<00:25,  3.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 47%|████▋     | 74/158 [00:21<00:24,  3.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 47%|████▋     | 75/158 [00:21<00:24,  3.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 48%|████▊     | 76/158 [00:22<00:23,  3.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 49%|████▊     | 77/158 [00:22<00:23,  3.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 49%|████▉     | 78/158 [00:22<00:23,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 79/158 [00:22<00:23,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 51%|█████     | 80/158 [00:23<00:21,  3.55it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 51%|█████▏    | 81/158 [00:23<00:21,  3.61it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 52%|█████▏    | 82/158 [00:23<00:21,  3.55it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 53%|█████▎    | 83/158 [00:24<00:20,  3.66it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 53%|█████▎    | 84/158 [00:24<00:20,  3.58it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 85/158 [00:24<00:20,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 86/158 [00:24<00:20,  3.55it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 55%|█████▌    | 87/158 [00:25<00:20,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 56%|█████▌    | 88/158 [00:25<00:20,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 56%|█████▋    | 89/158 [00:25<00:19,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 57%|█████▋    | 90/158 [00:26<00:19,  3.50it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 91/158 [00:26<00:18,  3.53it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 92/158 [00:26<00:18,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 59%|█████▉    | 93/158 [00:26<00:18,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 59%|█████▉    | 94/158 [00:27<00:18,  3.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 60%|██████    | 95/158 [00:27<00:18,  3.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 61%|██████    | 96/158 [00:27<00:17,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 61%|██████▏   | 97/158 [00:28<00:17,  3.59it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▏   | 98/158 [00:28<00:17,  3.53it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 63%|██████▎   | 99/158 [00:28<00:16,  3.57it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 63%|██████▎   | 100/158 [00:28<00:16,  3.58it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 64%|██████▍   | 101/158 [00:29<00:16,  3.50it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 65%|██████▍   | 102/158 [00:29<00:16,  3.47it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 65%|██████▌   | 103/158 [00:29<00:15,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 66%|██████▌   | 104/158 [00:30<00:15,  3.50it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 66%|██████▋   | 105/158 [00:30<00:15,  3.49it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 106/158 [00:30<00:14,  3.52it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 68%|██████▊   | 107/158 [00:30<00:14,  3.47it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 68%|██████▊   | 108/158 [00:31<00:14,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 69%|██████▉   | 109/158 [00:31<00:14,  3.44it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 70%|██████▉   | 110/158 [00:31<00:14,  3.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 70%|███████   | 111/158 [00:32<00:13,  3.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 112/158 [00:32<00:13,  3.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 72%|███████▏  | 113/158 [00:32<00:13,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 72%|███████▏  | 114/158 [00:32<00:12,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 73%|███████▎  | 115/158 [00:33<00:12,  3.44it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 73%|███████▎  | 116/158 [00:33<00:12,  3.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 74%|███████▍  | 117/158 [00:33<00:12,  3.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▍  | 118/158 [00:34<00:11,  3.52it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 119/158 [00:34<00:10,  3.62it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 76%|███████▌  | 120/158 [00:34<00:10,  3.57it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 77%|███████▋  | 121/158 [00:34<00:10,  3.55it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 77%|███████▋  | 122/158 [00:35<00:10,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 78%|███████▊  | 123/158 [00:35<00:10,  3.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 78%|███████▊  | 124/158 [00:35<00:09,  3.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 125/158 [00:36<00:09,  3.44it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|███████▉  | 126/158 [00:36<00:09,  3.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|████████  | 127/158 [00:36<00:09,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 81%|████████  | 128/158 [00:37<00:08,  3.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 82%|████████▏ | 129/158 [00:37<00:08,  3.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 82%|████████▏ | 130/158 [00:37<00:08,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 131/158 [00:37<00:07,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 84%|████████▎ | 132/158 [00:38<00:07,  3.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 84%|████████▍ | 133/158 [00:38<00:07,  3.23it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 85%|████████▍ | 134/158 [00:38<00:07,  3.05it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 85%|████████▌ | 135/158 [00:39<00:07,  3.15it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 86%|████████▌ | 136/158 [00:39<00:06,  3.19it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 87%|████████▋ | 137/158 [00:39<00:06,  3.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 87%|████████▋ | 138/158 [00:40<00:06,  3.25it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 139/158 [00:40<00:05,  3.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 89%|████████▊ | 140/158 [00:40<00:05,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 89%|████████▉ | 141/158 [00:40<00:04,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 90%|████████▉ | 142/158 [00:41<00:04,  3.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 91%|█████████ | 143/158 [00:41<00:04,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 91%|█████████ | 144/158 [00:41<00:04,  3.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 145/158 [00:42<00:03,  3.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 146/158 [00:42<00:03,  3.28it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 93%|█████████▎| 147/158 [00:42<00:03,  3.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 94%|█████████▎| 148/158 [00:43<00:02,  3.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 94%|█████████▍| 149/158 [00:43<00:02,  3.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 95%|█████████▍| 150/158 [00:43<00:02,  3.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 151/158 [00:43<00:02,  3.48it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 152/158 [00:44<00:01,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 97%|█████████▋| 153/158 [00:44<00:01,  3.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 97%|█████████▋| 154/158 [00:44<00:01,  3.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 98%|█████████▊| 155/158 [00:45<00:00,  3.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 99%|█████████▊| 156/158 [00:45<00:00,  3.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 99%|█████████▉| 157/158 [00:45<00:00,  3.25it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 158/158 [00:46<00:00,  3.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                      rouge1    rouge2    rougeL  rougeLsum\ntrained distilgpt2  0.004726  0.000305  0.004618   0.004683",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>trained distilgpt2</th>\n      <td>0.004726</td>\n      <td>0.000305</td>\n      <td>0.004618</td>\n      <td>0.004683</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('./distilgpt/finetuned_distilgpt2').cuda()\n",
    "score = calculate_metric_on_test_ds(\n",
    "    test_data, rouge_metric, model, tokenizer, batch_size = 1, column_text = 'article', column_summary= 'highlights'\n",
    ")\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "pd.DataFrame(rouge_dict, index = ['trained distilgpt2'] )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': \"(CNN) -- The partnership started as a single shop on Oxford Street in London, opened in 1864 by John Lewis. Today the partnership is an organization with bases throughout the UK, with supermarkets and department stores, employing approximately 67,100 people. All 67,100 permanent staff are Partners who own 26 John Lewis department stores, 183 Waitrose supermarkets, an online and catalogue business, John Lewis Direct a direct services company - Greenbee, three production units and a farm. Every Partner receives the same scale of bonus, based on a fixed percentage of their annual wage. The bonus for 2006 was 18% equivalent to 9 weeks pay, which was rolled out for every employee. Chairman Sir Stuart Hampson retired at the end of March 2007, his successor is Charlie Mayfield. Hampson's salary for January 26, 2006 to January 26, 2007 was $1.66 million which included the partnership bonus of $250,000. John Lewis' consolidated revenue for the last financial year was $11.4 billion.  E-mail to a friend .\", 'highlights': \"John Lewis Partnership began as a shop on London's Oxford street in 1864 .\\nAll 67,100 employees are partners in the organization and own shares .\", 'id': 'e117408ad19cc69e15b1e21b9ae54f10c07223ce', '__index_level_0__': 69, 'input_ids': [7, 18474, 8, 1377, 383, 10413, 2067, 355, 257, 2060, 6128, 319, 13643, 3530, 287, 3576, 11, 4721, 287, 1248, 2414, 416, 1757, 10174, 13, 6288, 262, 10413, 318, 281, 4009, 351, 12536, 3690, 262, 3482, 11, 351, 40875, 290, 5011, 7000, 11, 26490, 6702, 8275, 11, 3064, 661, 13, 1439, 8275, 11, 3064, 7748, 3085, 389, 14205, 508, 898, 2608, 1757, 10174, 5011, 7000, 11, 28551, 16314, 13698, 40875, 11, 281, 2691, 290, 34614, 1597, 11, 1757, 10174, 4128, 257, 1277, 2594, 1664, 532, 3469, 20963, 11, 1115, 3227, 4991, 290, 257, 5318, 13, 3887, 35532, 11583, 262, 976, 5046, 286, 7202, 11, 1912, 319, 257, 5969, 5873, 286, 511, 5079, 7699, 13, 383, 7202, 329, 4793, 373, 1248, 4, 7548, 284, 860, 2745, 1414, 11, 543, 373, 11686, 503, 329, 790, 6538, 13, 12787, 7361, 22559, 13039, 1559, 9880, 379, 262, 886, 286, 2805, 4343, 11, 465, 17270, 318, 11526, 1737, 3245, 13, 13039, 1559, 338, 9588, 329, 3269, 2608, 11, 4793, 284, 3269, 2608, 11, 4343, 373, 720, 16, 13, 2791, 1510, 543, 3017, 262, 10413, 7202, 286, 720, 9031, 11, 830, 13, 1757, 10174, 6, 27890, 6426, 329, 262, 938, 3176, 614, 373, 720, 1157, 13, 19, 2997, 13, 220, 412, 12, 4529, 284, 257, 1545, 764, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [7554, 10174, 23270, 2540, 355, 257, 6128, 319, 3576, 338, 13643, 4675, 287, 1248, 2414, 764, 198, 3237, 8275, 11, 3064, 4409, 389, 4887, 287, 262, 4009, 290, 898, 7303, 764, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259]}\n"
     ]
    }
   ],
   "source": [
    "print(train_data_pt[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}